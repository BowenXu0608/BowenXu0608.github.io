---
title: week 2 notes

---

---
layout: post
title: "week 2 notes"
date: 2025-11-22
---
# week 2 notes
The notes of the week 2 class about computer graphics.

## Part 1: What is the pipeline?
pipeline is a way people do their work in the aspect of computergraphics. pipeline starts at making raw meterial indepently and parallelly, then moves into the processes which have a fix order automaticly, finally we can get the outcome from the pipeline, just like the factory makes the sandwiches.

For computer graphics, the "raw material" are the geometries, light, textures, transform and many other things. the outcome are the rendered images. The processes between them is the pipeline, which can also be seen as a fuction. Because the whole screen can be seen as a huge table of the pixels, every pixel needs to confirm the strength of the red color, green color, blue color, and the opacity, the aim of the pipeline is giving a four-dimensional array for each pixel.

## Part 2: Five steps of the pipeline
In general, the process of pipeline can be divided into five steps: Application stage, Vertex stage, Rasterization stage, Fragment processing stage and Frame buffer operations stage. 

### First step: Application stage
In this step, the main aim is helping GPU understant the geometry. Unlike humans, GPU can't recognize the shape of the object. The only thing it can understand is the set of coordinate points. We can get some ideas from the .obj document. When you create a cube in the application and generate a .obj document, you may see the vertex data, vertex normal, texture coordinate, face or something else. These data can be used by the GPU to finish the next process.

### Second step: Vertex stage
Although the data of the geometries is sent to the GPU, there are still some problems existing. GPU will solve them by the calculation of the vertex. 

In this step we will use the vertex shader, "shader" here is not meaning of setting color, but the program of the GPU.

First of all, all the data for a singal geometry are recorded as the local position, so we need to place them to the right place in the whole world. Secondly, people conventionally put the camera on the origin point, make it look at -Z axis, and confirm the orientation of the upper part of the camera is y axis. To make sure the scene do not change, all the objects in the whole world need to follow the camera. This action will change the whole world to the camera space. Different from using SRT matrix to change the geometry from the origin point, we need to use the reverse sequence of it to take the geometry back. We can use the matrics below to finish it:
$T_{view}$=
$$
\begin{bmatrix}
1 & 0 & 0 & -x_e \\
0 & 1 & 0 & -y_e \\
0 & 0 & 1 & -z_e \\
0 & 0 & 0 & 1
\end{bmatrix}
$$


$R_{view}$=
$$
\begin{bmatrix}
x_{\hat{g}\times\hat{t}} & y_{\hat{g}\times\hat{t}} & z_{\hat{g}\times\hat{t}} & 0\\
x_t & y_t & z_t & 0\\
x_{-g} & y_{-g} & z_{-g} & 0\\
0 & 0 & 0 & 1
\end{bmatrix}
$$
Now the geometries are in the correct place. Next step we need to transform the space from camera space to the clip space. The way we doen it is using the projection. There are two kinds of projection are used. First of all, because we need to reflect the 3D model to a 2D screen, the distence between different objects should be recognized. Thus, the close objects should be looked bigger and the far objects should be looked smaller. The solution is when we calculate the geomatries using different scale of the plane, further geomatries will appear on a bigger plane. Following that, we can do the perspectire projection firstly. perspectire projection will squeeze the trapezoid to the cubiod, so that the far objects can become smaller. According to the properties of the similar triangle, we can finally get the matrix for perspectire projiction:
$$
\begin{bmatrix}
n & 0 & 0 & 0 \\
0 & n & 0 & 0 \\
0 & 0 & n+f & -nf \\
0 & 0 & 1 & 0
\end{bmatrix}
$$
After that, the shape of the spece has becomed a cubiod. we need to place it at origin and scale it to a $[-1,1]^3$. Assume the original cubiod is $[l,r]\times[b,t]\times[f,n]$, we need a scale matrix and a translation matrix to change it. Use a translation matrix to move the center of the cubiod to the origin first, then scale it to a cube with a side length of 2. As a result, the matrics below are what we need:
$M_s$= 
$$
\begin{bmatrix}
2/r-l & 0 & 0 & 0 \\
0 & 2/t-b & 0 & 0 \\
0 & 0 & 2/n-f & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
$$
$M_t$=
$$
\begin{bmatrix}
1 & 0 & 0 & -(r+l)/2 \\
0 & 1 & 0 & -(t+b)/2 \\
0 & 0 & 1 & -(n+f)/2 \\
0 & 0 & 0 & 1
\end{bmatrix}
$$
$M_{ortho}$=$M_s$$M_t$

Now the space is a cube. The last thing is deleting the z-axis so that the 3D space can be changed to a 2D graph, which we need in the next step.

### Third step: Rasterization stage
Now we get the precise data of the vertex, but we can't create graph only use the vertex. In this step, we will scale up the data size of the vertexout from O(V) to O(wh). That means we will change our view from every vertex to every pixel.

As we all know, different screen has different resolution ratio, such as 2k(2560 * 1440) or 4k(3840 * 2160). We have got a 1 * 1 graph now, and we need to scale it to the ratio of the screen, then split them into every pixel. In this step, we need to use the interpolation to calculate the data every pixel should have. For example, assume we have a triangle in the graph and a 2k screen. The first step is scaling it to adapt the screen, then we can split the graph into $2560\times1440$ squares, every square is a pixel. We can focus on the center point of each square, use the barycentric coordinate to confirm how the vertices influent this pixel, get the weight of each vertex. Moreover, we can check the proportion of triangle in each pixel to change the opacity, this process called "Anti-aliasing".

### Fourth step: Fragment processing stage
Before we start to understand the detail, we need to know why it called "fragment processing stage", or some people call it "pixel processing stage". The reason for "pixel" is that this step's aim is confirm the color of each pixel, and the reason for "fragment" is that the calculation is based on the every small part of the geomatries--we called that "fragment". Although not all the fragment will show on the screen finally, every fragment should be considered. In this step, every fragment's status will be confirmed. 

The first action is take the data from vertices, then use the weight calculated by step 3 to determine the data of the specific fragment. After this process, we can make sure all the data of each fragment, including color and opacity.

### fifth step: Frame buffer operations stage
The last step will concentrate on dealing with the multiple objects on the screen. When more than one object appear on the screen, there are some issues may emerge: if the stack order of the objects is important when the overlay of the objects exist? How should we calculate them? 

Here are three test will be do in this process: alpha test, depth test and stencil test. Firstly we put the data into frame buffer to conduct the color blending and finish the alpha test. If the opacity of the fragment is not enough, this fragment will be ignored. Following that, updated data will put into the depth buffer then do the depth test. If the fragment is not on the top, it will not be calculated. finally, the newest data will be put into the stencil buffer, and complete the stencil test. if the specific value of the fragment is not reaching the target value, this fragment will be ignored. After all the processes above, we can complete a rendering of an graph.